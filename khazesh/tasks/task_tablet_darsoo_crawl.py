# /home/bartardigi/scraping/khazesh/tasks/task_tablet_darsoo_crawl.py

import json
import re
import traceback
import requests
from celery import shared_task
from bs4 import BeautifulSoup
import logging
from khazesh.tasks.save_object_to_database import save_obj
from khazesh.tasks.save_crawler_status import update_code_execution_state
from requests.exceptions import ConnectionError, RequestException, Timeout
import time
from typing import List, Optional
import uuid
from khazesh.models import Mobile
from django.utils import timezone
from urllib.parse import urljoin, unquote
from html import unescape

# ------------------ Config ------------------
urls = [
    "https://darsoo.com/categories/%D8%AA%D8%A8%D9%84%D8%AA/%D8%AA%D8%A8%D9%84%D8%AA-%D8%B4%DB%8C%D8%A7%D8%A6%D9%88%D9%85%DB%8C/",
    "https://darsoo.com/categories/%D8%AA%D8%A8%D9%84%D8%AA/%D8%AA%D8%A8%D9%84%D8%AA-%D8%B3%D8%A7%D9%85%D8%B3%D9%88%D9%86%DA%AF/",
    "https://darsoo.com/categories/%D8%AA%D8%A8%D9%84%D8%AA/%D8%AA%D8%A8%D9%84%D8%AA-%D8%A7%D9%BE%D9%84/"
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

session = requests.Session()
session.headers.update(HEADERS)

SITE = 'Darsoo'
SELLER = 'Darsoo'


# ------------------ Safe request helper ------------------
def safe_get(url: str, retries: int = 2, delay: int = 1):
    """Safe HTTP GET with retries and error reporting"""
    for i in range(retries):
        try:
            response = session.get(url, timeout=20)
            response.raise_for_status()
            return response
        except (ConnectionError, Timeout) as ce:
            err = f"Connection error ({i+1}/{retries}): {ce}"
            print(f"âš ï¸ {url} - {err}")
            update_code_execution_state(f'{SITE}-tablet', False, err)
            time.sleep(delay)
        except RequestException as re:
            err = f"Request error: {re}"
            print(f"âš ï¸ {url} - {err}")
            update_code_execution_state(f'{SITE}-tablet', False, err)
            return None
        except Exception:
            err = traceback.format_exc()
            print(f"âš ï¸ {url} - {err}")
            update_code_execution_state(f'{SITE}-tablet', False, err)
            return None
    return None


# ------------------ Extract product info ------------------
def get_products_from_page(soup, base_url):
    """
    Ø³Ø§Ø®ØªØ§Ø± Ø¬Ø¯ÛŒØ¯ Ù„ÛŒØ³Øª Ù…Ø­ØµÙˆÙ„â€ŒÙ‡Ø§ Ø±ÙˆÛŒ Ø¯Ø§Ø±Ø³Ùˆ:
      <div class="childprocatbox">
        <a class="product-card-2" href="PRODUCT_URL"> ... <h3>TITLE</h3> ... <span class="woocommerce-Price-amount"><bdi>PRICE ØªÙˆÙ…Ø§Ù†</bdi></span> ... </a>
    """
    product_cards = soup.select("div.childprocatbox a.product-card-2")
    product_list = []

    for card in product_cards:
        try:
            link = card.get("href")
            h3 = card.find("h3")
            title_text = h3.get_text(strip=True) if h3 else None

            # Ù‚ÛŒÙ…Øª Ù…Ù…Ú©Ù†Ù‡ "Ù†Ø§Ù…ÙˆØ¬ÙˆØ¯" Ø¨Ø§Ø´Ù‡
            price_bdi = card.select_one(".woocommerce-Price-amount bdi")
            if price_bdi:
                raw_price = price_bdi.get_text(strip=True)
                # ØªØ¨Ø¯ÛŒÙ„ 23.386.000 â†’ 23386000
                price_num = re.sub(r"[^\d]", "", raw_price)
            else:
                # Ø§Ú¯Ø± Ù†Ø§Ù…ÙˆØ¬ÙˆØ¯ Ø¨ÙˆØ¯ØŒ price Ø±Ùˆ None Ù…ÛŒâ€ŒÚ¯Ø°Ø§Ø±ÛŒÙ… ØªØ§ ÙÛŒÙ„ØªØ± ÙØ¹Ù„ÛŒ Ø­ÙØ¸ Ø´ÙˆØ¯
                unavailable = card.select_one(".pcb-price span[style*='#df2d2d']")
                price_num = None if unavailable else None

            if title_text and link and price_num:
                full_link = urljoin(base_url, link)
                product_list.append({
                    "title": title_text,
                    "price": price_num,  # (Ø¨Ø§ Ø³ÛŒØ§Ø³Øª ÙØ¹Ù„ÛŒØŒ Ø¨Ø¹Ø¯Ø§Ù‹  *10 Ø¨Ø±Ø§ÛŒ Ø±ÛŒØ§Ù„)
                    "link": full_link
                })
        except Exception:
            continue

    print(f"ğŸ“¦ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø­ØµÙˆÙ„Ø§Øª ÛŒØ§ÙØªâ€ŒØ´Ø¯Ù‡ Ø¯Ø± {base_url}: {len(product_list)}")
    return product_list


def get_all_pagination_links(soup, base_url):
    """
    Ø§Ú¯Ø± ØµÙØ­Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ÙˆÚ©Ø§Ù…Ø±Ø³ Ø¨Ø§Ø´Ø¯ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ ul.page-numbers ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯.
    Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±Øª ÙÙ‚Ø· Ù‡Ù…Ø§Ù† base_url Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
    """
    pagination_links = []
    try:
        # ØªÙ„Ø§Ø´ 1: ÙˆÚ©Ø§Ù…Ø±Ø³
        for a in soup.select("ul.page-numbers li a"):
            href = a.get("href", "").strip()
            if href:
                full_link = urljoin(base_url, href)
                if full_link not in pagination_links:
                    pagination_links.append(full_link)

        # ØªÙ„Ø§Ø´ 2: Ù‡Ø± Ù†ÙˆØ¹ pagination Ø¯ÛŒÚ¯Ø±
        if not pagination_links:
            pagination_ul = soup.find("ul", class_="pagination")
            if pagination_ul:
                for li in pagination_ul.find_all("li"):
                    a_tag = li.find("a")
                    if a_tag and "href" in a_tag.attrs:
                        link = a_tag["href"].strip()
                        if link:
                            full_link = urljoin(base_url, link)
                            if full_link not in pagination_links:
                                pagination_links.append(full_link)
    except Exception:
        pass

    return pagination_links or [base_url]


def crawl_all_pages(base_url):
    response = safe_get(base_url)
    if not response:
        return []

    soup = BeautifulSoup(response.content, "html.parser")
    pagination_links = get_all_pagination_links(soup, base_url)
    all_products = []

    for page_url in pagination_links:
        print(f"ğŸ“„ Ø¯Ø± Ø­Ø§Ù„ Ú©Ø±ÙˆÙ„: {page_url}")
        response = safe_get(page_url)
        if not response:
            continue
        try:
            page_soup = BeautifulSoup(response.content, "html.parser")
            products = get_products_from_page(page_soup, page_url)
            all_products.extend(products)
            time.sleep(0.4)
        except Exception:
            update_code_execution_state(f'{SITE}-tablet', False, traceback.format_exc())
    return all_products


# ------------------ Product detail extraction ------------------
def get_full_product_title(soup):
    # Ø³Ø§Ø®ØªØ§Ø± Ø¬Ø¯ÛŒØ¯: h1.product-title
    try:
        h1 = soup.find("h1", class_="product-title")
        if h1:
            return h1.get_text(strip=True)
    except Exception:
        pass
    return "Ù†Ø§Ù…Ø´Ø®Øµ"


def _extract_color_hex_map(soup):
    """
    ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† Ø±Ù†Ú¯â€ŒÙ‡Ø§ Ùˆ Ú©Ø¯ Ø±Ù†Ú¯ Ø§Ø² UL Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø±Ù†Ú¯ Ø¯Ø± ØµÙØ­Ù‡ Ù…Ø­ØµÙˆÙ„.
    Ø§Ú¯Ø± Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ø®Ø§Ù„ÛŒ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯.
    """
    color_hex_map = {}
    try:
        for li in soup.select("ul.color-variable-items-wrapper li.variable-item"):
            val = li.get("data-value", "").strip()  # Ù…Ù…Ú©Ù†Ù‡ urlencoded ÛŒØ§ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ù…Ø«Ù„ 'blue' Ø¨Ø§Ø´Ù‡
            style_span = li.select_one(".variable-item-span-color")
            hex_code = None
            if style_span and style_span.has_attr("style"):
                m = re.search(r'background-color:\s*([^;]+)', style_span["style"])
                if m:
                    hex_code = m.group(1).strip()
            if val:
                color_hex_map[val] = hex_code or "Ù†Ø§Ù…Ø´Ø®Øµ"
    except Exception:
        pass
    return color_hex_map


def get_product_colors(product_url):
    """
    Ø³Ø§Ø®ØªØ§Ø± Ø¬Ø¯ÛŒØ¯: ÙØ±Ù… variations_form Ø¯Ø§Ø±Ø§ÛŒ data-product_variations Ø§Ø³Øª (JSON).
    Ø§Ø² Ø¢Ù†ØŒ Ù‚ÛŒÙ…ØªØŒ Ù…ÙˆØ¬ÙˆØ¯ÛŒØŒ Ø±Ù†Ú¯ (attribute_pa_color) Ùˆ Ú¯Ø§Ø±Ø§Ù†ØªÛŒ (attribute_pa_guarantee) Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†ÛŒÙ….
    """
    try:
        response = safe_get(product_url)
        if not response:
            return []

        soup = BeautifulSoup(response.content, "html.parser")
        form = soup.select_one("form.variations_form.cart")
        if not form:
            # fallback Ù‚Ø¯ÛŒÙ…ÛŒ
            price_tag = soup.find("span", class_="price")
            price = re.sub(r'\D', '', price_tag.text) if price_tag else "0"
            return [{
                "color_id": "Ù†Ø§Ù…Ø´Ø®Øµ",
                "color_code": "Ù†Ø§Ù…Ø´Ø®Øµ",
                "color_name": "Ù¾ÛŒØ´â€ŒÙØ±Ø¶",
                "price": int(price) * 10 if price else 0,
                "warranty": "Ù†Ø§Ù…Ø´Ø®Øµ"
            }]

        raw = form.get("data-product_variations", "")
        if not raw:
            return []

        # data-product_variations Ø¯Ø§Ø®Ù„ HTML escape Ø´Ø¯Ù‡ ( &quot; )
        variations = json.loads(unescape(raw))

        # Ø±Ù†Ú¯ â†’ Ú©Ø¯Ø±Ù†Ú¯ Ø§Ø² UL Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯
        color_hex_map = _extract_color_hex_map(soup)

        color_list = []
        for idx, var in enumerate(variations, 1):
            try:
                attrs = var.get("attributes", {})
                color_raw = attrs.get("attribute_pa_color", "")  # Ù…Ù…Ú©Ù†Ù‡ urlencoded Ø¨Ø§Ø´Ø¯
                guarantee_raw = attrs.get("attribute_pa_guarantee", "")

                color_name = unquote(color_raw) if color_raw else "Ù†Ø§Ù…Ø´Ø®Øµ"
                guarantee = unquote(guarantee_raw) if guarantee_raw else "Ù†Ø§Ù…Ø´Ø®Øµ"

                # Ù‚ÛŒÙ…Øª ØªÙˆÙ…Ø§Ù†ÛŒ Ø¯Ø± display_priceØ› Ø³ÛŒØ§Ø³Øª Ù‚Ø¨Ù„ÛŒ: *10 Ø¨Ø±Ø§ÛŒ Ø±ÛŒØ§Ù„ÛŒ
                display_price = var.get("display_price")
                price_rial = int(display_price) * 10 if isinstance(display_price, (int, float)) else 0

                # Ú©Ø¯ Ø±Ù†Ú¯ Ø§Ø² map (Ú©Ù„ÛŒØ¯ map Ù‡Ù…Ø§Ù† data-value Ø¯Ø± UL Ø§Ø³ØªØ› Ù…Ù…Ú©Ù† Ø§Ø³Øª urlencoded Ø¨Ø§Ø´Ø¯)
                color_hex = None
                if color_raw in color_hex_map:
                    color_hex = color_hex_map[color_raw]
                elif color_name in color_hex_map:
                    color_hex = color_hex_map[color_name]
                else:
                    color_hex = "Ù†Ø§Ù…Ø´Ø®Øµ"

                # ÙˆØ¶Ø¹ÛŒØª Ù…ÙˆØ¬ÙˆØ¯ÛŒ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
                in_stock = var.get("is_in_stock", False)

                color_list.append({
                    "color_id": str(var.get("variation_id") or idx),
                    "color_code": color_hex or "Ù†Ø§Ù…Ø´Ø®Øµ",
                    "color_name": color_name or "Ù†Ø§Ù…Ø´Ø®Øµ",
                    "price": price_rial,
                    "warranty": guarantee or "Ù†Ø§Ù…Ø´Ø®Øµ",
                    "in_stock": bool(in_stock),
                })
            except Exception:
                continue

        return color_list

    except Exception:
        update_code_execution_state(f'{SITE}-tablet', False, traceback.format_exc())
        return []


def get_brand_name_english(product_url):
    """
    Ù‚Ø¨Ù„Ø§Ù‹ Ø§Ø² breadcrumb Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯. Ø§Ú¯Ø± Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ø§Ø² Ø¹Ù†ÙˆØ§Ù† Ù„Ø§ØªÛŒÙ† Ø¯Ø§Ø®Ù„ Ø¹Ù†ÙˆØ§Ù† Ù…Ø­ØµÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
    """
    try:
        response = safe_get(product_url)
        if not response:
            return "Ù†Ø§Ù…Ø´Ø®Øµ"
        soup = BeautifulSoup(response.content, "html.parser")

        # ØªÙ„Ø§Ø´ 1: Ø³Ø§Ø®ØªØ§Ø± Ù‚Ø¨Ù„ÛŒ
        div = soup.find("div", class_="product-directory default")
        if div:
            ul = div.find("ul", class_="mr-1")
            if ul:
                for li in ul.find_all("li"):
                    span = li.find("span")
                    if span and "Ø¨Ø±Ù†Ø¯" in span.text:
                        a_tag = li.find("a")
                        if a_tag:
                            text = a_tag.text.strip()
                            if " - " in text:
                                return text.split(" - ")[1].strip()
                            return text

        # ØªÙ„Ø§Ø´ 2: Ø§Ø² Ø¹Ù†ÙˆØ§Ù†
        title = get_full_product_title(soup)
        # Ú†Ù†Ø¯ Ø¨Ø±Ù†Ø¯ Ø±Ø§ÛŒØ¬ Ø¨Ø±Ø§ÛŒ fallback
        brands = ["Xiaomi", "Samsung", "Apple", "Lenovo", "Huawei", "Honor", "Nokia", "Realme", "Oppo", "OnePlus"]
        for b in brands:
            if re.search(rf"\b{re.escape(b)}\b", title, flags=re.IGNORECASE):
                return b

        return "Ù†Ø§Ù…Ø´Ø®Øµ"
    except Exception:
        return "Ù†Ø§Ù…Ø´Ø®Øµ"


def normalize_gb(text):
    match = re.search(r'(\d+)', text)
    return f"{match.group(1)}GB" if match else text


def parse_title_info(title, brand_english):
    full_title = title.strip()
    try:
        if brand_english == "Ù†Ø§Ù…Ø´Ø®Øµ":
            model = "Ù†Ø§Ù…Ø´Ø®Øµ"
        else:
            pattern = re.compile(rf"({re.escape(brand_english)}.*?)(?:Ø¨Ø§ Ø­Ø§ÙØ¸Ù‡|Ø±Ù…|$)", re.IGNORECASE)
            match = pattern.search(full_title)
            model = match.group(1).strip() if match else full_title

        memory_match = re.search(r'(\d+\s*(?:Ú¯ÛŒÚ¯Ø§Ø¨Ø§ÛŒØª|Ú¯ÛŒÚ¯|GB|G))', full_title)
        memory = normalize_gb(memory_match.group(1)) if memory_match else "Ù†Ø§Ù…Ø´Ø®Øµ"

        ram_match = re.search(r'(\d+\s*(?:Ú¯ÛŒÚ¯Ø§Ø¨Ø§ÛŒØª|GB|G))\s*(?:RAM|Ø±Ù…)?', full_title, re.IGNORECASE)
        ram = normalize_gb(ram_match.group(1)) if ram_match else "Ù†Ø§Ù…Ø´Ø®Øµ"

        is_not_active = any(x in full_title.lower() for x in ["not active", "Ù†Ø§Øª Ø§Ú©ØªÛŒÙˆ"])

        return {"full_title": full_title, "model": model, "memory": memory, "ram": ram, "is_not_active": is_not_active}
    except Exception:
        return {"full_title": full_title, "model": full_title, "memory": "Ù†Ø§Ù…Ø´Ø®Øµ", "ram": "Ù†Ø§Ù…Ø´Ø®Øµ", "is_not_active": False}


def clean_model(text):
    patterns = [
        r'Ø¸Ø±ÙÛŒØª\s*\d+\s*(?:Ú¯ÛŒÚ¯Ø§Ø¨Ø§ÛŒØª|Ú¯ÛŒÚ¯|GB|G)',
        r'Ø±Ù…\s*\d+\s*(?:Ú¯ÛŒÚ¯Ø§Ø¨Ø§ÛŒØª|Ú¯ÛŒÚ¯|GB|G)',
        r'\d+\s*GB',
        r'(?:Ø­Ø§ÙØ¸Ù‡|Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ|Storage)',
    ]
    for pattern in patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    return re.sub(r'\s+', ' ', text).strip()


# ------------------ NEW: get_product_details (Ø·Ø¨Ù‚ Ø³Ø§Ø®ØªØ§Ø± ÙØ¹Ù„ÛŒ)
def get_product_details(product_url):
    try:
        response = session.get(product_url)
        if response.status_code != 200:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø­ØµÙˆÙ„ {product_url}: {response.status_code}")
            update_code_execution_state('Darsoo-tablet', False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø­ØµÙˆÙ„ {product_url}: {response.status_code}")
            return None

        soup = BeautifulSoup(response.content, "html.parser")
        full_title = get_full_product_title(soup)
        colors = get_product_colors(product_url)  # Ø§Ø² JSON variations Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯

        # ===== ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ ØªØ³Øª: Ú†Ø§Ù¾ Ø®Ø±ÙˆØ¬ÛŒ =====
        print(f"\nğŸ§¾ Ø¹Ù†ÙˆØ§Ù†: {full_title}")
        for c in colors:
            print(f"   â€¢ Ø±Ù†Ú¯: {c.get('color_name')}  | Ú©Ø¯Ø±Ù†Ú¯: {c.get('color_code')} | Ù‚ÛŒÙ…Øª (Ø±ÛŒØ§Ù„): {c.get('price')} | Ú¯Ø§Ø±Ø§Ù†ØªÛŒ: {c.get('warranty')} | Ù…ÙˆØ¬ÙˆØ¯ÛŒ: {c.get('in_stock')}")

        return {
            "full_title": full_title,
            "colors": colors,
            "soup": soup,
        }
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø²Ø¦ÛŒØ§Øª Ù…Ø­ØµÙˆÙ„ {product_url}: {e}")
        update_code_execution_state('Darsoo-tablet', False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø²Ø¦ÛŒØ§Øª Ù…Ø­ØµÙˆÙ„ {product_url}: {e}")
        return None


# ------------------ Main Task ------------------
@shared_task(bind=True, max_retries=1)
def tablet_darsoo_crawler(self):
    try:
        batch_id = f"Darsoo-{uuid.uuid4().hex[:12]}"
        all_tablet_objects = []
        total_found = 0

        for url in urls:
            print(f"ğŸŸ¡ Ø´Ø±ÙˆØ¹ Ú©Ø±ÙˆÙ„ Ø¯Ø³ØªÙ‡: {url}")
            products = crawl_all_pages(url)

            for p in products:
                try:
                    product_url = p["link"]
                    details = get_product_details(product_url)
                    if not details:
                        continue

                    full_title = details["full_title"]
                    colors = details["colors"]
                    brand_name_en = get_brand_name_english(product_url)
                    title_info = parse_title_info(full_title, brand_name_en)

                    # ÙÙ‚Ø· Ú†Ø§Ù¾ Ø¨Ø±Ø§ÛŒ ØªØ³Øª
                    print(f"ğŸ”— {product_url}")
                    print(f"ğŸ“Œ Ù…Ø¯Ù„ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒâ€ŒØ´Ø¯Ù‡: {clean_model(full_title)} | Ø¨Ø±Ù†Ø¯: {brand_name_en} | Ø­Ø§ÙØ¸Ù‡: {title_info['memory']} | Ø±Ù…: {title_info['ram']}")

                    for color_info in colors:
                        # Ø³Ø§Ø®Øª Ø¢Ø¨Ø¬Ú©Øª Ù…Ø·Ø§Ø¨Ù‚ Ø³Ø§Ø®ØªØ§Ø± ÙØ¹Ù„ÛŒ (Ø°Ø®ÛŒØ±Ù‡ Ù‡Ù†ÙˆØ² ÙØ¹Ø§Ù„Ù‡)
                        tablet_object = {
                            "model": clean_model(full_title),
                            "memory": title_info["memory"],
                            "ram": title_info["ram"],
                            "brand": brand_name_en,
                            "title": full_title,
                            "url": product_url,
                            "site": SITE,
                            "seller": SELLER,
                            "guarantee": color_info.get("warranty", "Ù†Ø§Ù…Ø´Ø®Øµ"),
                            "max_price": 1,
                            "mobile_digi_id": "",
                            "dual_sim": True,
                            "active": True,
                            "mobile": False,
                            "vietnam": False,
                            "not_active": title_info["is_not_active"],
                            "color_name": color_info.get("color_name"),
                            "color_hex": color_info.get("color_code"),
                            "min_price": color_info.get("price", 0),
                        }

                        all_tablet_objects.append(tablet_object)
                        total_found += 1

                except Exception:
                    update_code_execution_state(f'{SITE}-tablet', False, traceback.format_exc())
                    continue

        # ===== ÙÙ‚Ø· Ú†Ø§Ù¾ Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ ØªØ³Øª =====
        print(f"\nâœ… ØªØ¹Ø¯Ø§Ø¯ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒâ€ŒØ´Ø¯Ù‡: {total_found}")

        # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø«Ù„ Ù‚Ø¨Ù„ (Ø§Ú¯Ù‡ Ù†Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒ Ø§Ù„Ø§Ù† Ø°Ø®ÛŒØ±Ù‡ Ú©Ù†Ù‡ØŒ Ø§ÛŒÙ† Ø¯Ùˆ Ø¨Ù„Ø§Ú© Ø±Ùˆ Ú©Ø§Ù…Ù†Øª Ú©Ù†)
        for tablet_dict in all_tablet_objects:
            save_obj(tablet_dict, batch_id=batch_id)

        Mobile.objects.filter(site=SITE, mobile=False).exclude(last_batch_id=batch_id).update(status=False)
        update_code_execution_state(f'{SITE}-tablet', bool(all_tablet_objects), 'Ù‡ÛŒÚ† Ù…Ø­ØµÙˆÙ„ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.' if not all_tablet_objects else '')

    except Exception:
        error_message = traceback.format_exc()
        update_code_execution_state(f'{SITE}-tablet', False, error_message)
        print(f"Error {error_message}")
        raise self.retry(exc=Exception(error_message), countdown=30)
    finally:
        ten_min_ago = timezone.now() - timezone.timedelta(minutes=10)
        Mobile.objects.filter(site=SITE, status=True, mobile=False, updated_at__lt=ten_min_ago).update(status=False)
